# Introduction to the spsann package

## Spatial simulated annealing

### Search graph

The search graph corresponds to the set of effective candidate locations for
a point being jittered in a given iteration. The size of the search graph, 
i.e. the maximum distance that a point can be moved around, is correlated 
with the concept of **temperature**. A larger search graph is equivalent
to higher temperatures, which potentially result in more movement or 
"agitation"" of the set of points or "particles".

The current implementation of spatial simulated annealing uses a 
**linear cooling schedule** depending upon the number of iterations to 
control the size of the search graph. The equations are as follows:

```
x.max.b <- x.max.a - k / iterations * (x.max.a - x.min)
y.max.b <- y.max.a - k / iterations * (y.max.a - y.min)
```

where `x.max.a` and `y.max.a` are the maximum allowed shift in the
x and y coordinates in the current iteration, `x.min` and `y.min` 
are the minimum required shift in the x and y coordinates, and `x.max.b`
and `y.max.b` are the maximum allowed shift in the x and y coordinates
in the next iteration. `iterations` is the total number of iterations 
and `k` is the current iteration.

### Acceptance probability

The acceptance probability is the chance of accepting a new system 
configuration that is worse than the current system configuration. The 
concept of acceptance probability is related with that of 
**temperature**. A higher acceptance probability is equivalent to higher
temperatures, which potentially result in more movement or 
"agitation"" of the set of points or "particles".

Using a low initial acceptance probability turns the spatial simulated 
annealing into a *greedy* algorithm. It will converge in a shorter time,
but the solution found is likely to be a local optimum instead of the global 
optimum. Using a high initial acceptance probability ($>0.8$) usually is
the wisest choice.

An **exponential cooling schedule** depending upon the number of 
iterations is used in the current implementation of the spatial simulated
annealing to control the acceptance probability. The acceptance probability
at each iteration is calculates as follows:

```
actual_prob <- acceptance$initial * exp(-k / acceptance$cooling)
```

where `actual_prob` is the acceptance probability at the `k`-th 
iteration, `acceptance$initial` is the initial acceptance probability, 
and `acceptance$cooling` is the exponential cooling factor.

### Starting system configuration

Unidimensional criterion such as the number of points per lag distance class
are dependent on the starting system configuration by definition. This means
that, depending on the parameters passed to the spatial simulated annealing 
algorithm, many points will likely to stay close to their starting positions.
It would be reasonable to use a starting system configuration that is close 
to the global optimal, but such thing is not feasible.

Increasing the initial acceptance probability does not guarantee the 
independence from the starting system configuration. The most efficient 
option in the current implementation of the spatial simulated annealing
algorithm is to start using the entire spatial domain as search graph. This 
is set using the interval of the x and y coodinates to set `x.max` 
and `y.max` (See above).

An alternative is to start jittering (randomly perturbing) several points at
a time and use a cooling schedule to **exponentially** decrease the 
number of points jittered at each iteration. The current implementation of 
the spatial simulated annealing does not explore such alternative. The 
cooling schedule would be as follows:

```
new.size <- round(c(old.size - 1) * exp(-k / size.factor) + 1)
```

where `old.size` and `new.size` are the number of points jittered
in the previous and next iterations, `size.factor` is the cooling 
parameter, and `k` is the number of the current iteration. The larger 
the difference between the starting system configuration and the global 
optimum, the larger the number of points that would need to be jittered in
the first iterations. This will usually increase the time spent on the first
iterations.

### Number of iterations

The number of iterations has a large influence on the performance of the 
spatial simulated annealing algorithm. The larger the number of possible 
system configurations, the higher should the number of iterations be.

The number of possible system configurations increases with:

* a high initial acceptance probability
* the use of an infinite set of candidate locations
* the use of a very dense finite set of candidate locations

## Random perturbation (jittering)

This function perturbs the coordinates of spatial points adding random noise,
a process also known as 'jittering'. There are two ways of jittering 
the coordinates. They differ on how the the set of candidate locations is 
defined.

### Finite set of candidate locations

**NOTE**: The current implementation does not enable to define the direction of 
the perturbation, nor to perturb more than one point at a time.

The first method uses a finite set of candidate locations for the perturbed 
points. This method usually is the fastest because it does not require the 
use of complex routines to check if the perturbed point falls inside the 
spatial domain. Since the candidate locations is a finite set, any perturbed 
point will inexorably fall inside the spatial domain. This is a very 
important feature in optimization exercises with complex objective functions
such as simulated annealing when repetitive perturbation is required.

The arguments `x.min`, `y.min`, `x.max`, and `y.max` are used to define a 
rectangular window containing the set of effective candidate locations for the 
point defined with the argument `which.point`. The new location is then 
randomly sampled from the set of effective candidate locations and checked 
against existing points to avoid duplicates.

### Infinite set of candidate locations

**NOTE**: The current version does not accept using an infinite set of 
candidate locations.

The second method can be much slower than the first depending on the number of
points, on the shape of the area and on how the other arguments are set. This
method does not use a finite set of candidate locations. Instead, the number of
candidate locations is infinite. Its domain can be defined using the argument
`where`. The reason for the larger amount of time demanded is that the method 
has two internal steps to 1) check if the perturbed point falls inside the
spatial domain, and b) check if two of more points have coincident coordinates
(set using argument `zero`). Using an infinite set of candidate locations will
usually allow obtaining better results in optimization exercises such as 
spatial simulated annealing. However, the amount of time may be prohibitive
depending on the complexity of the problem.

The sub-argument `max` in both arguments `x.coord` and `y.coord` defines the
lower and upper limits of a uniform distribution:

```
runif(n, min = -max, max = max)
```

The quantity of noise added to the coordinates of the point being perturbed is
sampled from this uniform distribution. By default, the maximum quantity of
random noise added to the x and y coordinates is, respectively, equal to half 
the width and height of the bounding box of the set of points. This is 
equivalent to a vector **h** of length equal to half the diagonal of the 
bounding box. Therefore, a larger jittering is allowed in the longer coordinate
axis (x or y).

The direction of the perturbation is defined by the sign of the values sampled
from the uniform distribution. This means that the perturbation can assume any
direction from 0 to 360 degrees. By contrast, the function `jitter2d()` in the 
R-package **geoR** samples from a uniform distribution a value for the length 
of the vector **h** and a value for the direction of the perturbation.

`spJitter()` allows to set the minimum quantity of random noise added to a
coordinate with the sub-argument `min`. The absolute difference between the
original coordinate value and the jittered coordinate value is used to evaluate
this constraint. If the constraint is not met, `min` receives the sign of the
value sample from the uniform distribution and is added to the original 
coordinate value. This does not guarantee that the perturbation will be in the
same direction, but in the same quadrant.

When a spatial domain is defined, `spJitter()` evaluates if the perturbed 
points fall inside it using the function \code{\link[rgeos]{gContains}} from 
the R-package **rgeos**. All points falling outside the spatial domain are
identified and have their original coordinates jittered one again. Every new
coordinate falling inside the spatial domain is accepted. Every point falling
outside the spatial domain has its coordinates jittered till it falls inside 
the spatial domain. The number of iterations necessary to meet this constraint
depends on the complexity of the shape of the spatial domain. `spJitter()` 
tries to speed up the process by linearly decreasing the maximum quantity of
noise added to the coordinates at each iteration. If the number of iterations 
was not enough to guarantee all points inside the spatial domain, `spJitter()`
returns the jittered SpatialPoints with a warning message informing how many
points do not meet the constraint.

## Objective functions

Here we provide a description of the implementation of the objective functions 
in \strong{spsann}. We also describe the utopia and nadir points, which can 
help in the construction of multi-objective optimization problems.

### Points (or point-pairs) per lag distance class - PPL

#### Distances

Euclidean distances between points are calculated. This computation requires
the coordinates to be projected. The user is responsible for making sure that
this requirement is attained.

#### Distribution

Using the default uniform distribution means that the number of **point-pairs**
per lag distance class is equal to $n \times (n - 1) / (2 \times lag)$, where
$n$ is the total number of points in `points`, and $lag$ is the number of lag
distance classes.

Using the default uniform distribution means that the number of **points** per
lag distance class is equal to the total number of points in `points`. This is
the same as expecting that each point contributes to every lag distance class.

Distributions other that the available options can be easily implemented 
changing the arguments `lags`, `lags.base` and `pre.distri`.

#### Type of lags

Two types of lag distance classes can be created by default. The first 
(`lags.type = "equidistant"`), are evenly spaced lags. They are created by
simply dividing the distance interval from 0.0001 to `cutoff` by the required
number of lags. The minimum value of 0.0001 guarantees that a point does not
form a pair with itself.

The second type (`lags.type = "exponential"`) of lag distance classes is defined
by exponential spacings. The spacings are defined by the base $b$ of the
exponential expression $b^n$, where $n$ is the required number of lags. The base
is defined using argument `lags.base`. For example, the default `lags.base = 2`
creates lags that are sequentially defined as half of the immediately preceding
larger lag. If `cutoff = 100` and `lags = 4`, the upper limits of the lag
distance classes will be

```
> 100 / (2 ^ c(1:4))
[1] 50.00 25.00 12.50  6.25
```
#### Criteria

The functions `objPoints` and `objPairs` (to be implemented) were designed to be
used in spatial simulated annealing to optimize spatial sample configurations.
Both of them have two criteria implemented. The first is called using 
`criterion = "distribution"` and is used to minimize the sum of differences
between a pre-specified distribution and the observed distribution of points or
point-pairs per lag distance class.

Consider that we aim at having the following distribution of points per lag: 

```{r}
desired <- c(10, 10, 10, 10, 10)
```

and that the observed distribution of points per lag is the following:

```{r}
observed <- c(1, 2, 5, 10, 10)
```

The objective at each iteration of the optimization will be to match the two 
distributions. This criterion is of the same type as the one proposed by 
Warrick and Myers (1987).

The second criterion is called using `criterion = "minimum"`. It corresponds to
maximizing the minimum number of points or point-pairs observed over all lag
distance classes. Consider we observe the following distribution of points per
lag in the first iteration:

```{r}
observed <- c(1, 2, 5, 10, 10)
```

The objective in the next iteration will be to increase the number of points
in the first lag ($n = 1$). Consider we then have the following resulting
distribution:

```{r}
resulting <- c(5, 2, 5, 10, 10)
```

Now the objective will be to increse the number of points in the second lag
($n = 2$). The optimization continues until it is not possible to increase the
number of points in any of the lags, that is, when:

```{r}
distribution <- c(10, 10, 10, 10, 10)
```

This shows that the result of using `criterion = "minimum"` is similar to using
`criterion = "distribution"`. However, the resulting sample pattern can be
significantly different. The running time of each iteration can be a bit longer
when using `criterion = "distribution"`, but since it is a more sensitive
criteriom (it takes all lags into account), convergence is likely to be
attained with a smaller number of iterations. Note that this also depends on
the other parameters passed to the optimization algorithm.

It is important to note that using the first criterion (`"distribution"`) in
simulated annealing corresponds to a **minimization** problem. On the other 
hand, using the second criterion (`"minimum"`) would correspond to a 
**maximization** problem. We solve this inconsistency substituting the criterion
that has to be maximized by its inverse. For conveninence we multiply the
resulting value by a constant (i.e. $c / x + 1$, where $c$ is the number of
points and $x$ is the criterion value). This procedure allows us to define both
problems as minimization problems.

#### Utopia and nadir points

When `criterion = "distribution"`, the **utopia** ($f^{\circ}_{i}$) point is
exactly zero ($f^{\circ}_{i} = 0$). When `criterion = "minimum"`, the utopia
point is approximately 1 (0.9) ($f^{\circ}_{i} \sim 1$). It can be calculated
using the equation $n / n + 1$, where $n$ is the number of points (`objPoints`),
or the number point-pairs divided by the number of lag distance classes
(`objPairs`).

The **nadir** ($f^{max}_{i}$) point depends on a series of elements. For
instance, when `criterion = "distribution"`, if the desired distribution of
point or point-pairs per lag distance class is 

```{r}
pre.distribution <- c(10, 10, 10, 10, 10)
```

the worst case scenario would be to have all points or point-pairs in a single
lag distance class, that is, 

```{r}
obs.distribution <- c(0, 50, 0, 0, 0)
```

In this case, the nadir point is equal to the sum of the differences between the
two distributions:

```
sum((c(10, 10, 10, 10, 10) - c(0, 50, 0, 0, 0)) ^ 2) = 2000
```

When `objective = "minimum"`, the nadir point is equal to 
$f^{max}_{i} = n / 0 + 1 = n$.

### Association/correlation and marginal distribution of the covariates - ACDC

### Mean squared shortest distance - MSSD

#### Distances

Euclidean distances between points are calculated. This computation requires
the coordinates to be projected. The user is responsible for making sure that
this requirement is attained.

#### Matrix of distances

Calculating the matrix of Euclidean distances between all sample points and all 
prediction locations is computationally expensive. As such, the full matrix of 
distances is calculated only once for the initial system configuration before 
the first iteration. At each iteration, only the distance between the new 
sample point and all prediction locations is calculated. This numeric vector is
used to replace the column of the matrix of distances which contained the 
distances between the old jittered sample point and all prediction locations. 
The mean squared shortest distance of the new system configuration is then 
calculated using the updated matrix of distances. The whole proceedure is done 
at the C++-level to speed-up the computation.

#### Utopia and nadir points

The MSSD is a bi-dimensional criterion because it explicitly takes 
into account both y and x coordinates. It aims at the spread of points in 
the geographic space. This is completely different from the number of points
per lag distance class which is an uni-dimensional criterion -- it aims 
at the spread on points in the variogram space. It is more difficult to 
calculate the utopia and nadir points of a bi-dimensional criterion.

The **utopia** ($f^{\circ}_{i}$) point of MSSD is only known to be larger than
zero. It could be approximated using the k-means algorithm, which is much 
faster than spatial simulated annealing, but does not guarantee to return the
true utopia point. The **nadir** ($f^{max}_{i}$) point is obtained when all
sample points are clustered in one of the "corners"" of the spatial domain. 
This cannot be calculated and has to be approximated by simulation or using the
knowledge of the diagonal of the spatial domain (the maximum possible distance
between two points).

One alternative strategy is to first optimize a set of sample points using the
MSSD as criterion and then create geographic strata. In the multi-objective
optimization one would then have to define an unidimensional criterion aiming 
at matching the optimal solution obtained by the minimization of the MSSD. One
such uni-dimensional criterion would be the difference between the expected
distribution and the observed distribution of sample points per geographic 
strata. This criterion would aim at having at least one point per geographic
strata -- this is similar to optimizing sample points using the number of 
points per lag distance class.

A second uni-dimensional criterion would be the difference between the expected
MSSD and the observed MSSD. This criterion would aim at having the points
coinciding with the optimal solution obtained by the minimization of the MSSD. 
In both cases the utopia point would be exactly zero ($f^{\circ}_{i} = 0$). The
nadir point could be easily calculated for the first uni-dimensional criterion,
but not for the second.

### Mean (or maximum) universal kriging variance - MUKV
