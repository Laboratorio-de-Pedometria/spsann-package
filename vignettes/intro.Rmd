# Introduction to the spsann package

## Spatial simulated annealing

### Search graph

The search graph corresponds to the set of effective candidate locations for
a point being jittered in a given iteration. The size of the search graph,
i.e. the maximum distance that a point can be moved around, is correlated
with the concept of **temperature**. A larger search graph is equivalent
to higher temperatures, which potentially result in more movement or
"agitation"" of the set of points or "particles".

The current implementation of spatial simulated annealing uses a
**linear cooling schedule** depending upon the number of iterations to
control the size of the search graph. The equations are as follows:

```
x.max.b <- x.max.a - k / iterations * (x.max.a - x.min)
y.max.b <- y.max.a - k / iterations * (y.max.a - y.min)
```

where `x.max.a` and `y.max.a` are the maximum allowed shift in the
x and y coordinates in the current iteration, `x.min` and `y.min`
are the minimum required shift in the x and y coordinates, and `x.max.b`
and `y.max.b` are the maximum allowed shift in the x and y coordinates
in the next iteration. `iterations` is the total number of iterations
and `k` is the current iteration.

### Acceptance probability

The acceptance probability is the chance of accepting a new system
configuration that is worse than the current system configuration. The
concept of acceptance probability is related with that of
**temperature**. A higher acceptance probability is equivalent to higher
temperatures, which potentially result in more movement or
"agitation"" of the set of points or "particles".

Using a low initial acceptance probability turns the spatial simulated
annealing into a *greedy* algorithm. It will converge in a shorter time,
but the solution found is likely to be a local optimum instead of the global
optimum. Using a high initial acceptance probability ($>0.8$) usually is
the wisest choice.

An **exponential cooling schedule** depending upon the number of
iterations is used in the current implementation of the spatial simulated
annealing to control the acceptance probability. The acceptance probability
at each iteration is calculates as follows:

```
actual_prob <- acceptance$initial * exp(-k / acceptance$cooling)
```

where `actual_prob` is the acceptance probability at the `k`-th
iteration, `acceptance$initial` is the initial acceptance probability,
and `acceptance$cooling` is the exponential cooling factor.

### Starting system configuration

Unidimensional criterion such as the number of points per lag distance class
are dependent on the starting system configuration by definition. This means
that, depending on the parameters passed to the spatial simulated annealing
algorithm, many points will likely to stay close to their starting positions.
It would be reasonable to use a starting system configuration that is close
to the global optimal, but such thing is not feasible.

Increasing the initial acceptance probability does not guarantee the
independence from the starting system configuration. The most efficient
option in the current implementation of the spatial simulated annealing
algorithm is to start using the entire spatial domain as search graph. This
is set using the interval of the x and y coordinates to set `x.max`
and `y.max` (See above).

An alternative is to start jittering (randomly perturbing) several points at
a time and use a cooling schedule to **exponentially** decrease the
number of points jittered at each iteration. The current implementation of
the spatial simulated annealing does not explore such alternative. The
cooling schedule would be as follows:

```
new.size <- round(c(old.size - 1) * exp(-k / size.factor) + 1)
```

where `old.size` and `new.size` are the number of points jittered
in the previous and next iterations, `size.factor` is the cooling
parameter, and `k` is the number of the current iteration. The larger
the difference between the starting system configuration and the global
optimum, the larger the number of points that would need to be jittered in
the first iterations. This will usually increase the time spent on the first
iterations.

### Number of iterations

The number of iterations has a large influence on the performance of the
spatial simulated annealing algorithm. The larger the number of possible
system configurations, the higher should the number of iterations be.

The number of possible system configurations increases with:

* a high initial acceptance probability
* the use of an infinite set of candidate locations
* the use of a very dense finite set of candidate locations

## Random perturbation (jittering)

This function perturbs the coordinates of spatial points adding random noise,
a process also known as 'jittering'. There are two ways of jittering
the coordinates. They differ on how the the set of candidate locations is
defined.

### Finite set of candidate locations

**NOTE**: The current implementation does not enable to define the direction of
the perturbation, nor to perturb more than one point at a time.

The first method uses a finite set of candidate locations for the perturbed
points. This method usually is the fastest because it does not require the
use of complex routines to check if the perturbed point falls inside the
spatial domain. Since the candidate locations is a finite set, any perturbed
point will inexorably fall inside the spatial domain. This is a very
important feature in optimization exercises with complex objective functions
such as simulated annealing when repetitive perturbation is required.

The arguments `x.min`, `y.min`, `x.max`, and `y.max` are used to define a
rectangular window containing the set of effective candidate locations for the
point defined with the argument `which.point`. The new location is then
randomly sampled from the set of effective candidate locations and checked
against existing points to avoid duplicates.

### Infinite set of candidate locations

**NOTE**: The current version does not accept using an infinite set of
candidate locations.

The second method can be much slower than the first depending on the number of
points, on the shape of the area and on how the other arguments are set. This
method does not use a finite set of candidate locations. Instead, the number of
candidate locations is infinite. Its domain can be defined using the argument
`where`. The reason for the larger amount of time demanded is that the method
has two internal steps to 1) check if the perturbed point falls inside the
spatial domain, and b) check if two of more points have coincident coordinates
(set using argument `zero`). Using an infinite set of candidate locations will
usually allow obtaining better results in optimization exercises such as
spatial simulated annealing. However, the amount of time may be prohibitive
depending on the complexity of the problem.

The sub-argument `max` in both arguments `x.coord` and `y.coord` defines the
lower and upper limits of a uniform distribution:

```
runif(n, min = -max, max = max)
```

The quantity of noise added to the coordinates of the point being perturbed is
sampled from this uniform distribution. By default, the maximum quantity of
random noise added to the x and y coordinates is, respectively, equal to half
the width and height of the bounding box of the set of points. This is
equivalent to a vector **h** of length equal to half the diagonal of the
bounding box. Therefore, a larger jittering is allowed in the longer coordinate
axis (x or y).

The direction of the perturbation is defined by the sign of the values sampled
from the uniform distribution. This means that the perturbation can assume any
direction from 0 to 360 degrees. By contrast, the function `jitter2d()` in the
R-package **geoR** samples from a uniform distribution a value for the length
of the vector **h** and a value for the direction of the perturbation.

`spJitter()` allows to set the minimum quantity of random noise added to a
coordinate with the sub-argument `min`. The absolute difference between the
original coordinate value and the jittered coordinate value is used to evaluate
this constraint. If the constraint is not met, `min` receives the sign of the
value sample from the uniform distribution and is added to the original
coordinate value. This does not guarantee that the perturbation will be in the
same direction, but in the same quadrant.

When a spatial domain is defined, `spJitter()` evaluates if the perturbed
points fall inside it using the function \code{\link[rgeos]{gContains}} from
the R-package **rgeos**. All points falling outside the spatial domain are
identified and have their original coordinates jittered one again. Every new
coordinate falling inside the spatial domain is accepted. Every point falling
outside the spatial domain has its coordinates jittered till it falls inside
the spatial domain. The number of iterations necessary to meet this constraint
depends on the complexity of the shape of the spatial domain. `spJitter()`
tries to speed up the process by linearly decreasing the maximum quantity of
noise added to the coordinates at each iteration. If the number of iterations
was not enough to guarantee all points inside the spatial domain, `spJitter()`
returns the jittered SpatialPoints with a warning message informing how many
points do not meet the constraint.

## Objective functions

Here we provide a description of the implementation of the objective functions
in \strong{spsann}. We also describe the utopia and nadir points, which can
help in the construction of multi-objective optimization problems.

### Points (or point-pairs) per lag distance class - PPL

#### Distances

Euclidean distances between points are calculated. This computation requires
the coordinates to be projected. The user is responsible for making sure that
this requirement is attained.

#### Distribution

Using the default uniform distribution means that the number of **point-pairs**
per lag distance class is equal to $n \times (n - 1) / (2 \times lag)$, where
$n$ is the total number of points in `points`, and $lag$ is the number of lag
distance classes.

Using the default uniform distribution means that the number of **points** per
lag distance class is equal to the total number of points in `points`. This is
the same as expecting that each point contributes to every lag distance class.

Distributions other that the available options can be easily implemented
changing the arguments `lags`, `lags.base` and `pre.distri`.

#### Type of lags

Two types of lag distance classes can be created by default. The first
(`lags.type = "equidistant"`), are evenly spaced lags. They are created by
simply dividing the distance interval from 0.0001 to `cutoff` by the required
number of lags. The minimum value of 0.0001 guarantees that a point does not
form a pair with itself.

The second type (`lags.type = "exponential"`) of lag distance classes is defined
by exponential spacings. The spacings are defined by the base $b$ of the
exponential expression $b^n$, where $n$ is the required number of lags. The base
is defined using argument `lags.base`. For example, the default `lags.base = 2`
creates lags that are sequentially defined as half of the immediately preceding
larger lag. If `cutoff = 100` and `lags = 4`, the upper limits of the lag
distance classes will be

```
> 100 / (2 ^ c(1:4))
[1] 50.00 25.00 12.50  6.25
```
#### Criteria

The functions `objPoints` and `objPairs` (to be implemented) were designed to be
used in spatial simulated annealing to optimize spatial sample configurations.
Both of them have two criteria implemented. The first is called using
`criterion = "distribution"` and is used to minimize the sum of differences
between a pre-specified distribution and the observed distribution of points or
point-pairs per lag distance class.

Consider that we aim at having the following distribution of points per lag:

```{r}
desired <- c(10, 10, 10, 10, 10)
```

and that the observed distribution of points per lag is the following:

```{r}
observed <- c(1, 2, 5, 10, 10)
```

The objective at each iteration of the optimization will be to match the two
distributions. This criterion is of the same type as the one proposed by
Warrick and Myers (1987).

The second criterion is called using `criterion = "minimum"`. It corresponds to
maximizing the minimum number of points or point-pairs observed over all lag
distance classes. Consider we observe the following distribution of points per
lag in the first iteration:

```{r}
observed <- c(1, 2, 5, 10, 10)
```

The objective in the next iteration will be to increase the number of points
in the first lag ($n = 1$). Consider we then have the following resulting
distribution:

```{r}
resulting <- c(5, 2, 5, 10, 10)
```

Now the objective will be to increase the number of points in the second lag
($n = 2$). The optimization continues until it is not possible to increase the
number of points in any of the lags, that is, when:

```{r}
distribution <- c(10, 10, 10, 10, 10)
```

This shows that the result of using `criterion = "minimum"` is similar to using
`criterion = "distribution"`. However, the resulting sample pattern can be
significantly different. The running time of each iteration can be a bit longer
when using `criterion = "distribution"`, but since it is a more sensitive
criterion (it takes all lags into account), convergence is likely to be
attained with a smaller number of iterations. Note that this also depends on
the other parameters passed to the optimization algorithm.

It is important to note that using the first criterion (`"distribution"`) in
simulated annealing corresponds to a **minimization** problem. On the other
hand, using the second criterion (`"minimum"`) would correspond to a
**maximization** problem. We solve this inconsistency substituting the criterion
that has to be maximized by its inverse. For convenience we multiply the
resulting value by a constant (i.e. $c / x + 1$, where $c$ is the number of
points and $x$ is the criterion value). This procedure allows us to define both
problems as minimization problems.

#### Utopia and nadir points

When `criterion = "distribution"`, the **utopia** ($f^{\circ}_{i}$) point is
exactly zero ($f^{\circ}_{i} = 0$). When `criterion = "minimum"`, the utopia
point is approximately 1 (0.9) ($f^{\circ}_{i} \sim 1$). It can be calculated
using the equation $n / n + 1$, where $n$ is the number of points (`objPoints`),
or the number point-pairs divided by the number of lag distance classes
(`objPairs`).

The **nadir** ($f^{max}_{i}$) point depends on a series of elements. For
instance, when `criterion = "distribution"`, if the desired distribution of
point or point-pairs per lag distance class is

```{r}
pre.distribution <- c(10, 10, 10, 10, 10)
```

the worst case scenario would be to have all points or point-pairs in a single
lag distance class, that is,

```{r}
obs.distribution <- c(0, 50, 0, 0, 0)
```

In this case, the nadir point is equal to the sum of the differences between the
two distributions:

```
sum((c(10, 10, 10, 10, 10) - c(0, 50, 0, 0, 0)) ^ 2) = 2000
```

When `objective = "minimum"`, the nadir point is equal to
$f^{max}_{i} = n / 0 + 1 = n$.

### Association/correlation and marginal distribution of the covariates - ACDC

The current version of `optimACDC()` accepts either *numeric* or *factor*
covariates. We define the covariates as *numeric* and *factor* covariates
instead of *continuous* and *categorical* covariates because the last is not an
appropriate definition. We show bellow that this is specially important for
*numeric* covariates.

#### Numeric covariates

Numeric covariates are commonly known as continuous covariates. A continuous
covariate is a covariate that may assume any value in the interval between its
minimum and maximum values. This is not the case of spatial data, which is
constrained by its geographic limits. As such, not all mathematically possible
values in the interval between its minimum and maximum values exist. We are
constrained to the existing values. The result is that we have a discontinuous
covariate. This is why Minasny and McBratney (2006) called their method
*constrained Latin hypercube sampling*. By contrast, a Latin hypercube sample
considers the entire probability distribution function of a covariate. As such,
any value is attainable.

The use of numeric covariates requires the definition of sampling strata. Two
types of sampling strata can be defined: *equal area* and *equal range*.
Minasny and McBratney (2006) suggested using *equal area* strata so that the
sample would reproduce the marginal distribution of each covariate. The number
of strata was set to the number of sample points so that the sample would be
marginally maximally stratified. The definition of the strata was based on the
quantiles of the covariate.

An important problem of the procedure proposed by Minasny and McBratney (2006),
as seen in its implementation in the R-package **clhs** by Roudier et al.
(2012), is that it assumed the covariates to be continuous. As such, the lower
and upper limits of the various strata can assume values that do not exist in
the covariate. The example bellow shows that although we can estimate 5 sampling
strata for a covariate composed of 15 values, many of the break points do not
exist in the covariate, such as 2.6 and 4.4:

```{r, message=FALSE}
n.pts <- 5
covars <- data.frame(x = c(1, 5, 1, 3, 4, 1, 2, 3, 2, 1, 8, 9, 9, 9, 9))
probs <- seq(0, 1, length.out = n.pts + 1)
breaks <- lapply(covars, quantile, probs, na.rm = TRUE)
breaks
```

`optimACDC()` solves this problem using a discontinuous function to calculate
the quantiles - see *Definition 3* of Hyndman & Fan (1996), which is 
implemented in the function `quantile(..., type = 3)`. This function was chosen 
because it produced the best break points for a set of covariates with 
different distributions. It was the only one to produce the exact break points  
for a uniformly distributed variable. For the example above, the quantiles
now honour the fact that the covariate is numeric but discontinuous:

```{r, message=FALSE}
breaks <- lapply(covars, quantile, probs, na.rm = TRUE, type = 3)
breaks
```

We also see that the *equal area* method has an inherent problem. Because we
want the sample to reproduce the marginal distribution of the covariate, a
break point may be repeated if that value has a high probability density
in the covariate. This is what happens with the values 1 and 9. The result is
that we can only have three strata instead of five:

```{r, message=FALSE}
breaks <- lapply(breaks, unique)
breaks
```

Because we want to have five sample points while matching the marginal
distribution of the covariate, the number of points per strata will be
non-uniform. We solve this counting the number of points of the covariate that
fall in each of the new strata:

```{r, message=FALSE}
n_cov <- ncol(covars)
count <- lapply(1:n_cov, function (i)
  hist(covars[, i], breaks[[i]], plot = FALSE)$counts)
count <- lapply(1:n_cov, function(i) count[[i]] / sum(count[[i]]) * n.pts)
count
```

The *equal range* method also has limitations when the covariate is
discontinuous. Like the *equal area* method, the lower and upper limits of the
various strata can assume values that do not exist in the covariate. It also 
may create strata containing values that do not exist in the covariate. We see 
both problems bellow:

```{r, message=FALSE}
n_cov <- ncol(covars)
breaks <- lapply(1:n_cov, function(i)
  seq(min(covars[, i]), max(covars[, i]), length.out = n.pts + 1))
breaks
count <- lapply(1:n_cov, function (i)
  hist(covars[, i], breaks[[i]], plot = FALSE)$counts)
count
```

It is clear that the break points calculated are inappropriate for our
covariate. The fourth strata accommodates values that do not exist in the
covariate. The current version of `optimACDC()` solves the first problem 
finding the nearest neighbour to each calculated break point. Next we check for 
possible duplicates as it is done for the *equal area* method. Duplicates are 
likely to appear when the number of sample points is larger than the number 
unique values in the covariate.

```{r, message=FALSE}
require(SpatialTools)
d <- lapply(1:n_cov, function(i)
  dist2(matrix(breaks[[i]]), matrix(covars[, i])))
d <- lapply(1:n_cov, function(i) apply(d[[i]], 1, which.min))
breaks <- lapply(1:n_cov, function(i) breaks[[i]] <- covars[d[[i]], i])
breaks <- lapply(breaks, unique)
breaks
```

We use the same strategy used with the *equal area* method to calculate the
distribution of sample points per strata. The resulting distribution is exactly
proportional to the existing distribution of points per strata in the 
covariate. As such, non-integer values are commonly obtained:

```{r, message=FALSE}
count <- lapply(1:n_cov, function (i)
  hist(covars[, i], breaks[[i]], plot = FALSE)$counts)
count <- lapply(1:n_cov, function(i)
  count[[i]] / sum(count[[i]]) * n.pts)
count
```

##### Geographic coordinates

The geographic coordinates can also be used to optimize the sample pattern
setting `use.coords = TRUE`. Each of them is taken as a numeric covariate as
any other numeric covariate passed to `optimACDC()`. If the used wishes to use
only the geographic coordinates, then they should be passed using the argument
`covars`.

One must bear in mind that using the geographic coordinates as covariates is 
not the same as optimizing a sample pattern minimizing the mean squared 
shortest distance (MSSD) implemented in `optimMSSD()`. The claim of Minasny & 
McBratney (2006) that their method (cLHS) could take the geographic space into
account seem to have been the responsible for the misunderstanding. ACDC (and 
cLHS) is an optimizing criterion concerned with the marginal distribution of 
the covariates, while MSSD is close to the joint distribution of the 
covariates. The first aims at trend estimation, the second at spatial 
interpolation. Using the geographic coordinates with `optimACDC()` will result 
in a sample pattern that is sub-optimal for spatial interpolation. For a square 
study area, one possible solution of `optimACDC()` is to locate all sample 
points in the diagonal.

```{r cLHS, message=FALSE, fig.width=4, fig.height=4}
# Using geographic coordinates as covariates
# Possible solution given by optimACDC()
require(ggplot2)
x <- y <- 1:5
qplot(x, y, asp = 1, main = "Coordinates as covariates")
```

We recommend that the geographic coordinates be used to optimize the sample
pattern only if they are to be included in trend estimation. This is useful if 
there is an evident geographic trend in the data. Use `optimMSSD()` to optimize
a sample pattern for spatial interpolation.

##### Correlation

We should compute the true population correlation matrix (pcm) and
then compare it with the sample correlation matrix (scm). Currently, we 
calculate both correlation matrices as being sample correlation matrices.

#### Factor covariates

*Factor* covariates are commonly known as categorical covariates. We prefer to 
use the term *factor* because this is the term used in R. Factor covariates 
include area-class soil maps, geological maps, land use maps, etc.. The user 
must bear in mind that the current version of `optimACDC()` does not accept 
numeric and factor covariates for the optimization. This can be seen as a 
weakness, since the original method of Minasny and McBratney (2006) that was 
implemented by Roudier et al. (2012) in the R-package **clhs** accepts both 
numeric and factor covariates. We explain bellow the reason for the change.

The original method of Minasny and McBratney (2006) was formulated as a 
multi-objective optimization problem. Three objective functions were set. The
first (O1) was defined so that the sample would reproduce the marginal 
distribution of the numeric covariates. The second (O2) was concerned with the 
factor covariates. The goal was to obtain a sample that would reproduce the 
class proportions. The third (O3) was defined to guarantee that the linear 
structure present in the population would be reproduced in the sample. The 
Pearson correlation coefficient was used as a measure of this linear structure.

Two of the objective functions (O1 and O3) were designed to guarantee that the 
sample would reproduce the features of the numeric covariates. This means that 
the method inherently gives a larger weight to the numeric covariates. One 
could view this feature as positive, claiming that the numeric covariates have 
more information that the factor covariates. We disagree with this view because 
"information content" is not related with explanatory power: the factor
covariates might be better predictors than the numeric ones. As such, we see 
this as a bias. This cannot be corrected by choosing proper weights because the 
information available usually is insuficient to do so.

The best solution would be to use a measure of association among factor
covariates as the linear correlation is used for the numeric covariates. The 
Cramér's *V* can be used for this purpose. The Cramér's *V* is a measure of 
association between factor covariates that ranges from 0 to 1: the closer to 1, 
the larger the association between two factor covariates. The Cramér's *V* is 
given by (Cramér, 1946):

$$V = \sqrt{\frac{\chi^{2}/n}{min(c-1, r-1)}}$$

where $r$ and $c$ are the number of rows and columns of the contingency table, $n$ is the number of observations, and $\chi^{2}$ is the chi-squared statistic.

We do not have a solution to measure the association/correlation among the 
numeric and factor covariates. As such, we choose to transform any numeric 
covariate into a factor covariate, and then use the Cramér's *V* is a measure 
of association. The numeric covariates are categorized using the sampling 
strata defined above (equal area or equal range). This solution is not without 
losses. While the Pearson's correlation coefficient shows the degree and 
direction of the association between two covariates (negative or positive), the 
Cramér's *V* only measures the degree (weak or strong). We did not make any 
test to check if there is any negative consequence deriving from our choice. If 
any, they might become lew important as the number of sample points increases.

#### Utopia and nadir points

The utopia and nadir points can be easily calculated if there is only one 
covariate. For the (more commom) case in which there are many covariates, the 
utopia and nadir points have to be estimated using expert knowledge or 
simulations.

### Mean squared shortest distance - MSSD

#### Distances

Euclidean distances between points are calculated. This computation requires
the coordinates to be projected. The user is responsible for making sure that
this requirement is attained.

#### Matrix of distances

Calculating the matrix of Euclidean distances between all sample points and all
prediction locations is computationally expensive. As such, the full matrix of
distances is calculated only once for the initial system configuration before
the first iteration. At each iteration, only the distance between the new
sample point and all prediction locations is calculated. This numeric vector is
used to replace the column of the matrix of distances which contained the
distances between the old jittered sample point and all prediction locations.
The mean squared shortest distance of the new system configuration is then
calculated using the updated matrix of distances. The whole procedure is done
at the C++-level to speed-up the computation.

#### Utopia and nadir points

The MSSD is a bi-dimensional criterion because it explicitly takes
into account both y and x coordinates. It aims at the spread of points in
the geographic space. This is completely different from the number of points
per lag distance class which is an uni-dimensional criterion -- it aims
at the spread on points in the variogram space. It is more difficult to
calculate the utopia and nadir points of a bi-dimensional criterion.

The **utopia** ($f^{\circ}_{i}$) point of MSSD is only known to be larger than
zero. It could be approximated using the k-means algorithm, which is much
faster than spatial simulated annealing, but does not guarantee to return the
true utopia point. The **nadir** ($f^{max}_{i}$) point is obtained when all
sample points are clustered in one of the "corners"" of the spatial domain.
This cannot be calculated and has to be approximated by simulation or using the
knowledge of the diagonal of the spatial domain (the maximum possible distance
between two points).

One alternative strategy is to first optimize a set of sample points using the
MSSD as criterion and then create geographic strata. In the multi-objective
optimization one would then have to define an unidimensional criterion aiming
at matching the optimal solution obtained by the minimization of the MSSD. One
such uni-dimensional criterion would be the difference between the expected
distribution and the observed distribution of sample points per geographic
strata. This criterion would aim at having at least one point per geographic
strata -- this is similar to optimizing sample points using the number of
points per lag distance class.

A second uni-dimensional criterion would be the difference between the expected
MSSD and the observed MSSD. This criterion would aim at having the points
coinciding with the optimal solution obtained by the minimization of the MSSD.
In both cases the utopia point would be exactly zero ($f^{\circ}_{i} = 0$). The
nadir point could be easily calculated for the first uni-dimensional criterion,
but not for the second.

### Mean (or maximum) universal kriging variance - MUKV
